{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 랭킹 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T02:54:22.754584Z",
     "start_time": "2021-01-25T02:54:22.428468Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.daum.net/ranking/popular?regDate=20210118\" \n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    다음 뉴스의 랭킹 뉴스는 \"많이 본\", \"열독률 높은\", \"댓글 많은\", \"연령별\" 로 나뉘어져 있다. 뉴스 url에서 tag로 나뉘어져 있는데 https://news.daum.net/ranking/ 뒤에 각각 \"popular\", \"kkomkkom\", \"bestreply\", \"age\"로 구분지어져 있다. 각각의 tag의 뉴스 구성은 전혀 다르게 되어 있어 따로 크롤링을 해야 한다. 그 외에도 연예, 스포츠 뉴스를 따로 구분하나 지금 당장 필요하진 않으므로 굳이 크롤링 해오진 않을 계획이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:32:35.369100Z",
     "start_time": "2021-01-21T13:32:35.365513Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 뉴스 태그 구분\n",
    "tag = [\"popular\", \"kkomkkom\", \"bestreply\", \"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T01:09:54.301247Z",
     "start_time": "2021-01-22T01:09:36.811509Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.1 popular tag 뉴스만 가져오기\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.daum.net/ranking/popular?regDate=20210118\" \n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "\n",
    "# 랭크, 언론사, URL, 제목 가져오기\n",
    "ranking_box = soup.find_all(class_ = \"rank_num rank_popular\")\n",
    "contents_box = soup.find_all(class_ = \"cont_thumb\")\n",
    "l = []\n",
    "\n",
    "for num in range(50):\n",
    "    d = {}\n",
    "    d['Rank'] = ranking_box[num].find(class_ = \"screen_out\").get_text()\n",
    "    \n",
    "    new_info = contents_box[num].find(class_ =\"tit_thumb\")\n",
    "    d['Press'] = new_info.find(class_ = 'info_news').get_text()\n",
    "    d['URL'] = new_info.find('a')['href']\n",
    "    d['Title'] = new_info.find('a').get_text()\n",
    "    l.append(d)\n",
    "\n",
    "# 기사 요약본, 메인 텍스트 가져오기\n",
    "for link in l:\n",
    "    resp = requests.get(link['URL'])\n",
    "    soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "    contents = soup.find(class_ = \"news_view\")\n",
    "    link['Main Article'] = contents.find(class_ = \"article_view\").get_text()\n",
    "    \n",
    "df = pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-22T01:09:27.685684Z",
     "start_time": "2021-01-22T01:09:27.080147Z"
    },
    "code_folding": [
     0,
     11,
     37,
     63,
     90
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.2 popular, kkomkkom, bestreply, age tag 뉴스만 가져오기\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 뉴스 태그 구분\n",
    "# tag = [\"popular\", \"kkomkkom\", \"bestreply\", \"age\"]\n",
    "tag = [\"popular\", \"kkomkkom\", \"bestreply\", \"age\"]\n",
    "\n",
    "\n",
    "def popular(soup):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num rank_popular\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(50):\n",
    "        d = {}\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "def kkomkkom(soup):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num rank_popular\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(30):\n",
    "        d = {}\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "def bestreply(soup):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(30):\n",
    "        d = {}\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "        d['Comment'] = ranking_box[num].find(class_=\"ico_news2\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "def age(soup):\n",
    "    # 여성과 남성의 ranking news 모으기\n",
    "    female = soup.find_all(class_=\"rank_female\")\n",
    "    male = soup.find_all(class_=\"rank_male\")\n",
    "    ranking_news = [female, male]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for news in ranking_news:     # female -> male\n",
    "        for i in range(4):        # 20s -> 50s\n",
    "            l = []\n",
    "            age = news[i].find(class_=\"txt_news\").get_text()[:3]\n",
    "            sex = news[i].find(class_=\"txt_news\").get_text()[4:]\n",
    "            press_list = news[i].find_all(class_=\"info_news\")\n",
    "            news_list = news[i].find_all(class_=\"link_txt\")\n",
    "\n",
    "            for num in range(5):\n",
    "                d = {}\n",
    "                d['Age'] = age\n",
    "                d['Sex'] = sex\n",
    "                d['Rank'] = num + 1\n",
    "                d['Press'] = press_list[num].get_text()\n",
    "                d['URL'] = news_list[num]['href']\n",
    "                d['Title'] = news_list[num].get_text()\n",
    "                l.append(d)\n",
    "\n",
    "            for link in l:\n",
    "                resp = requests.get(link['URL'])\n",
    "                soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "                contents = soup.find(class_=\"news_view\")\n",
    "                link['Main Article'] = contents.find(\n",
    "                    class_=\"article_view\").get_text()\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame(l)], axis=0, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "url = \"https://news.daum.net/ranking/age?regDate=20210118\"\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "# popular = popular(soup)\n",
    "# kkomkkom = kkomkkom(soup)\n",
    "# bestreply = bestreply(soup)\n",
    "# age = age(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T02:31:19.393108Z",
     "start_time": "2021-01-25T02:31:18.638128Z"
    },
    "code_folding": [
     14
    ]
   },
   "outputs": [],
   "source": [
    "# demo v0.3 날짜 입력을 받아 랭킹 뉴스 자동으로 긁어오기\n",
    "\n",
    "# 경로 및 tag 설정\n",
    "import os\n",
    "os.chdir(r\"C:/Users/cjy89/NLP/Project_news_crawling/\")\n",
    "tags = [\"popular\", \"kkomkkom\", \"bestreply\", \"age\"]\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 많이 본 순\n",
    "def popular(soup, date):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num rank_popular\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(50):\n",
    "        d = {}\n",
    "        d['Date'] = int(date)\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "# 열독률 높은 순\n",
    "def kkomkkom(soup, date):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num rank_popular\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(30):\n",
    "        d = {}\n",
    "        d['Date'] = int(date)\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "# 댓글 많은 순\n",
    "def bestreply(soup, date):\n",
    "    # 랭크, 언론사, URL, 제목 가져오기\n",
    "    ranking_box = soup.find_all(class_=\"rank_num\")\n",
    "    contents_box = soup.find_all(class_=\"cont_thumb\")\n",
    "    l = []\n",
    "\n",
    "    for num in range(30):\n",
    "        d = {}\n",
    "        d['Date'] = int(date)\n",
    "        d['Rank'] = ranking_box[num].find(class_=\"screen_out\").get_text()\n",
    "        d['Comment'] = ranking_box[num].find(class_=\"ico_news2\").get_text()\n",
    "\n",
    "        new_info = contents_box[num].find(class_=\"tit_thumb\")\n",
    "        d['Press'] = new_info.find(class_='info_news').get_text()\n",
    "        d['URL'] = new_info.find('a')['href']\n",
    "        d['Title'] = new_info.find('a').get_text()\n",
    "        l.append(d)\n",
    "\n",
    "    # 기사 요약본, 메인 텍스트 가져오기\n",
    "    for link in l:\n",
    "        resp = requests.get(link['URL'])\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        contents = soup.find(class_=\"news_view\")\n",
    "        link['Main Article'] = contents.find(class_=\"article_view\").get_text()\n",
    "\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "# 나이, 성별\n",
    "def age(soup, date):\n",
    "    # 여성과 남성의 ranking news 모으기\n",
    "    female = soup.find_all(class_=\"rank_female\")\n",
    "    male = soup.find_all(class_=\"rank_male\")\n",
    "    ranking_news = [female, male]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for news in ranking_news:     # female -> male\n",
    "        for i in range(4):        # 20s -> 50s\n",
    "            l = []\n",
    "            age = news[i].find(class_=\"txt_news\").get_text()[:3]\n",
    "            sex = news[i].find(class_=\"txt_news\").get_text()[4:]\n",
    "            press_list = news[i].find_all(class_=\"info_news\")\n",
    "            news_list = news[i].find_all(class_=\"link_txt\")\n",
    "\n",
    "            for num in range(5):\n",
    "                d = {}\n",
    "                d['Date'] = int(date)\n",
    "                d['Age'] = age\n",
    "                d['Sex'] = sex\n",
    "                d['Rank'] = num + 1\n",
    "                d['Press'] = press_list[num].get_text()\n",
    "                d['URL'] = news_list[num]['href']\n",
    "                d['Title'] = news_list[num].get_text()\n",
    "                l.append(d)\n",
    "\n",
    "            for link in l:\n",
    "                resp = requests.get(link['URL'])\n",
    "                soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "                contents = soup.find(class_=\"news_view\")\n",
    "                link['Main Article'] = contents.find(\n",
    "                    class_=\"article_view\").get_text()\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame(l)], axis=0, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# main interface function\n",
    "def get_ranking_news(date):\n",
    "    total_time = 0\n",
    "    for tag in tags:\n",
    "        start = time.time()\n",
    "        url = \"https://news.daum.net/ranking/\" + tag + \"?regDate=\" + str(date)\n",
    "        resp = requests.get(url)\n",
    "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
    "        \n",
    "        if(tag == \"popular\"):\n",
    "            df = popular(soup, date)\n",
    "        elif(tag ==\"kkomkkom\"):\n",
    "            df = kkomkkom(soup, date)\n",
    "        elif(tag == \"bestreply\"):\n",
    "            df = bestreply(soup, date)\n",
    "        elif(tag == \"age\"):\n",
    "            df = age(soup, date)\n",
    "        \n",
    "        title = \"Daum/\" + tag + \"/\" + str(date) + \"_ranking_news.csv\"\n",
    "        df.to_csv(title, sep=\",\", index=False, encoding=\"utf-8-sig\")\n",
    "        end = time.time()\n",
    "        total_time += end-start\n",
    "        print(\"Crawling \" + str(date) + \" \" + tag + \" news : \", end-start)\n",
    "    print(\"Total time :\", total_time)\n",
    "    print(\"Average time : \", total_time/4)\n",
    "    print(\"───────────────────\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T02:53:56.635330Z",
     "start_time": "2021-01-25T02:38:31.799382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 20210101 popular news :  14.202004194259644\n",
      "Crawling 20210101 kkomkkom news :  8.70995044708252\n",
      "Crawling 20210101 bestreply news :  6.056759595870972\n",
      "Crawling 20210101 age news :  10.833054780960083\n",
      "Total time : 39.80176901817322\n",
      "Average time :  9.950442254543304\n",
      "───────────────────\n",
      "Crawling 20210102 popular news :  14.436276912689209\n",
      "Crawling 20210102 kkomkkom news :  8.053711891174316\n",
      "Crawling 20210102 bestreply news :  6.168506860733032\n",
      "Crawling 20210102 age news :  10.931079149246216\n",
      "Total time : 39.58957481384277\n",
      "Average time :  9.897393703460693\n",
      "───────────────────\n",
      "Crawling 20210103 popular news :  14.312278270721436\n",
      "Crawling 20210103 kkomkkom news :  8.491061210632324\n",
      "Crawling 20210103 bestreply news :  6.082691192626953\n",
      "Crawling 20210103 age news :  10.851634502410889\n",
      "Total time : 39.7376651763916\n",
      "Average time :  9.9344162940979\n",
      "───────────────────\n",
      "Crawling 20210104 popular news :  13.139715433120728\n",
      "Crawling 20210104 kkomkkom news :  8.563136577606201\n",
      "Crawling 20210104 bestreply news :  6.3893256187438965\n",
      "Crawling 20210104 age news :  10.09870982170105\n",
      "Total time : 38.190887451171875\n",
      "Average time :  9.547721862792969\n",
      "───────────────────\n",
      "Crawling 20210105 popular news :  13.17903733253479\n",
      "Crawling 20210105 kkomkkom news :  8.248955965042114\n",
      "Crawling 20210105 bestreply news :  5.9525673389434814\n",
      "Crawling 20210105 age news :  10.814477443695068\n",
      "Total time : 38.195038080215454\n",
      "Average time :  9.548759520053864\n",
      "───────────────────\n",
      "Crawling 20210106 popular news :  13.133303880691528\n",
      "Crawling 20210106 kkomkkom news :  9.291443824768066\n",
      "Crawling 20210106 bestreply news :  6.056220293045044\n",
      "Crawling 20210106 age news :  10.318679332733154\n",
      "Total time : 38.79964733123779\n",
      "Average time :  9.699911832809448\n",
      "───────────────────\n",
      "Crawling 20210107 popular news :  13.003812789916992\n",
      "Crawling 20210107 kkomkkom news :  8.808768033981323\n",
      "Crawling 20210107 bestreply news :  6.7689831256866455\n",
      "Crawling 20210107 age news :  9.926621437072754\n",
      "Total time : 38.508185386657715\n",
      "Average time :  9.627046346664429\n",
      "───────────────────\n",
      "Crawling 20210108 popular news :  12.108303546905518\n",
      "Crawling 20210108 kkomkkom news :  7.871140480041504\n",
      "Crawling 20210108 bestreply news :  6.06275749206543\n",
      "Crawling 20210108 age news :  10.296921253204346\n",
      "Total time : 36.3391227722168\n",
      "Average time :  9.0847806930542\n",
      "───────────────────\n",
      "Crawling 20210109 popular news :  14.275720357894897\n",
      "Crawling 20210109 kkomkkom news :  8.19763731956482\n",
      "Crawling 20210109 bestreply news :  5.921587705612183\n",
      "Crawling 20210109 age news :  10.705388307571411\n",
      "Total time : 39.10033369064331\n",
      "Average time :  9.775083422660828\n",
      "───────────────────\n",
      "Crawling 20210110 popular news :  13.855988502502441\n",
      "Crawling 20210110 kkomkkom news :  8.776512384414673\n",
      "Crawling 20210110 bestreply news :  6.021113872528076\n",
      "Crawling 20210110 age news :  12.110809564590454\n",
      "Total time : 40.764424324035645\n",
      "Average time :  10.191106081008911\n",
      "───────────────────\n",
      "Crawling 20210111 popular news :  13.626603364944458\n",
      "Crawling 20210111 kkomkkom news :  8.518476486206055\n",
      "Crawling 20210111 bestreply news :  8.185878992080688\n",
      "Crawling 20210111 age news :  11.020879030227661\n",
      "Total time : 41.35183787345886\n",
      "Average time :  10.337959468364716\n",
      "───────────────────\n",
      "Crawling 20210112 popular news :  13.493004560470581\n",
      "Crawling 20210112 kkomkkom news :  8.201410293579102\n",
      "Crawling 20210112 bestreply news :  6.24514627456665\n",
      "Crawling 20210112 age news :  10.24478006362915\n",
      "Total time : 38.18434119224548\n",
      "Average time :  9.54608529806137\n",
      "───────────────────\n",
      "Crawling 20210113 popular news :  14.184371709823608\n",
      "Crawling 20210113 kkomkkom news :  7.837209463119507\n",
      "Crawling 20210113 bestreply news :  6.469598770141602\n",
      "Crawling 20210113 age news :  10.3168363571167\n",
      "Total time : 38.808016300201416\n",
      "Average time :  9.702004075050354\n",
      "───────────────────\n",
      "Crawling 20210114 popular news :  12.891592502593994\n",
      "Crawling 20210114 kkomkkom news :  8.091892957687378\n",
      "Crawling 20210114 bestreply news :  6.2444798946380615\n",
      "Crawling 20210114 age news :  10.473777770996094\n",
      "Total time : 37.70174312591553\n",
      "Average time :  9.425435781478882\n",
      "───────────────────\n",
      "Crawling 20210115 popular news :  13.722447156906128\n",
      "Crawling 20210115 kkomkkom news :  8.852334260940552\n",
      "Crawling 20210115 bestreply news :  6.371523141860962\n",
      "Crawling 20210115 age news :  10.844357967376709\n",
      "Total time : 39.79066252708435\n",
      "Average time :  9.947665631771088\n",
      "───────────────────\n",
      "Crawling 20210116 popular news :  14.128233671188354\n",
      "Crawling 20210116 kkomkkom news :  8.216940879821777\n",
      "Crawling 20210116 bestreply news :  6.312605619430542\n",
      "Crawling 20210116 age news :  10.7512788772583\n",
      "Total time : 39.409059047698975\n",
      "Average time :  9.852264761924744\n",
      "───────────────────\n",
      "Crawling 20210117 popular news :  13.558711051940918\n",
      "Crawling 20210117 kkomkkom news :  7.895875453948975\n",
      "Crawling 20210117 bestreply news :  5.975045204162598\n",
      "Crawling 20210117 age news :  10.638550519943237\n",
      "Total time : 38.06818222999573\n",
      "Average time :  9.517045557498932\n",
      "───────────────────\n",
      "Crawling 20210118 popular news :  13.010189056396484\n",
      "Crawling 20210118 kkomkkom news :  8.528069972991943\n",
      "Crawling 20210118 bestreply news :  6.127866983413696\n",
      "Crawling 20210118 age news :  11.464090824127197\n",
      "Total time : 39.13021683692932\n",
      "Average time :  9.78255420923233\n",
      "───────────────────\n",
      "Crawling 20210119 popular news :  12.599057912826538\n",
      "Crawling 20210119 kkomkkom news :  9.569020509719849\n",
      "Crawling 20210119 bestreply news :  6.399756908416748\n",
      "Crawling 20210119 age news :  9.942293882369995\n",
      "Total time : 38.51012921333313\n",
      "Average time :  9.627532303333282\n",
      "───────────────────\n",
      "Crawling 20210120 popular news :  13.644370794296265\n",
      "Crawling 20210120 kkomkkom news :  8.249564170837402\n",
      "Crawling 20210120 bestreply news :  6.228945255279541\n",
      "Crawling 20210120 age news :  9.904680013656616\n",
      "Total time : 38.027560234069824\n",
      "Average time :  9.506890058517456\n",
      "───────────────────\n",
      "Crawling 20210121 popular news :  12.76511287689209\n",
      "Crawling 20210121 kkomkkom news :  7.88711953163147\n",
      "Crawling 20210121 bestreply news :  6.908782482147217\n",
      "Crawling 20210121 age news :  9.427005290985107\n",
      "Total time : 36.988020181655884\n",
      "Average time :  9.247005045413971\n",
      "───────────────────\n",
      "Crawling 20210122 popular news :  12.727153539657593\n",
      "Crawling 20210122 kkomkkom news :  8.400689601898193\n",
      "Crawling 20210122 bestreply news :  5.749301195144653\n",
      "Crawling 20210122 age news :  9.649195432662964\n",
      "Total time : 36.5263397693634\n",
      "Average time :  9.13158494234085\n",
      "───────────────────\n",
      "Crawling 20210123 popular news :  13.20701289176941\n",
      "Crawling 20210123 kkomkkom news :  7.851912260055542\n",
      "Crawling 20210123 bestreply news :  6.389633893966675\n",
      "Crawling 20210123 age news :  11.256319046020508\n",
      "Total time : 38.704878091812134\n",
      "Average time :  9.676219522953033\n",
      "───────────────────\n",
      "Crawling 20210124 popular news :  11.697371244430542\n",
      "Crawling 20210124 kkomkkom news :  7.771823406219482\n",
      "Crawling 20210124 bestreply news :  5.757267475128174\n",
      "Crawling 20210124 age news :  9.34841775894165\n",
      "Total time : 34.57487988471985\n",
      "Average time :  8.643719971179962\n",
      "───────────────────\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    get_ranking_news(20210101 + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 기사 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T02:55:22.408364Z",
     "start_time": "2021-01-25T02:55:22.404420Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "import os\n",
    "os.chdir(r\"C:/Users/cjy89/NLP/Project_news_crawling/Daum/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
